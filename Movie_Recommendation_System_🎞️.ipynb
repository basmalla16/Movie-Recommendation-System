{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 6663,
          "sourceType": "datasetVersion",
          "datasetId": 3405
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Movie Recommendation System ðŸŽžï¸",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "rounakbanik_the_movies_dataset_path = kagglehub.dataset_download('rounakbanik/the-movies-dataset')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "CvwuVq1bhf59"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¦¾Different Types of Recommendation Systems :"
      ],
      "metadata": {
        "id": "9IpsTeIMhf6P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is likely that you will concur that there are multiple approaches to determining what to suggest or recommend when a friend seeks our opinion. This principle is equally applicable to artificial intelligence."
      ],
      "metadata": {
        "id": "6XcvEw8Thf6V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the realm of `machine learning`, three predominant techniques for constructing recommendation engines are\n",
        "1. Content-based filtering.\n",
        "2. Collaborative filtering.\n",
        "3. Hybrid filtering."
      ],
      "metadata": {
        "id": "DzbAZ3bjhf6X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1- content-based filtering:\n",
        "This method generates suggestions *based on items* you have previously liked. It uses historical data such as purchase records and search history to identify similar products.\n",
        "\n",
        "> For example, if you rated the movie \"Inception\" with five stars, the system will recommend similar movies such as \"Interstellar\".\n",
        "\n",
        "![image.png](attachment:36e37efe-734b-44e0-addf-56e148240783.png)\n",
        "\n",
        "However, if all recommendation systems solely relied on your viewing history, discovering new genres and films would be challenging. This is where **Collaborative filtering** comes into play. But what exactly does it entail?."
      ],
      "metadata": {
        "id": "Gkf8rKJUhf6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2- Collaborative filtering:\n",
        "This method identifies other users who have similar preferences to you, and recommends items **based on their choices**.\n",
        "\n",
        "> For example, if you and your friend both like the movie \"The Shawshank Redemption\", and your friend also likes \"Forrest Gump\", the system will recommend \"Forrest Gump\" to you.\n",
        "\n",
        "![image.png](attachment:c069599b-e451-4388-befb-b8a251d590da.png)"
      ],
      "metadata": {
        "id": "urKQ80Myhf6b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following sections, I will show you how to build a movie recommendation engine using Python and **collaborative filtering techniques**."
      ],
      "metadata": {
        "id": "2pqmxlSdhf6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary:\n",
        "![image.png](attachment:45c4702a-aa37-401f-b6c8-8703db115be1.png)"
      ],
      "metadata": {
        "id": "3nxEEdYuhf6f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![image.png](attachment:54a4168b-86ce-4141-bf38-9bcbeb1b9fa7.png)"
      ],
      "metadata": {
        "id": "7xsn5nCnhf6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "_____"
      ],
      "metadata": {
        "id": "p2GZpzgbhf6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*I'm going to attempt to create a Recommendation Systems based on `collaborative filtering` using `matrix factorization` to get all my embeddings straight and then I'm going to top it off with a `k-means` clustering algorithm to get my prediction to see in general how well i have done collaborative filtering .*"
      ],
      "metadata": {
        "id": "fz1rxabGhf6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "____"
      ],
      "metadata": {
        "id": "Af7FKgQ0hf6j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What Is Matrix Factorization ??\n",
        "\n",
        "[*Matrix Factorization*](https://medium.com/@ilyurek/understanding-matrix-factorization-a-simple-guide-20e2b32989eb) is an advanced **collaborative filtering** technique used to solve the problem of missing data (such as movies that have not been rated by a user yet). We divide the large rating matrix into smaller matrices that represent the relationships between users and movies.\n",
        "\n",
        "**How does it work?**\n",
        "\n",
        "We take a rating matrix **R** where the rows contain users and the columns contain movies. The matrix stores the users' ratings for the movies. If a user has not rated a particular movie, this cell will be empty. Matrix analysis breaks this matrix into:\n",
        "\n",
        "User matrix **U**: Each user is represented by a vector that reflects their hidden preferences.\n",
        "Movie matrix **V**: Each movie is represented by a vector that reflects the hidden characteristics associated with that movie.\n",
        "\n",
        "We multiply these two matrices to recover the original rating matrix **R**.\n",
        "\n",
        "The main idea is to predict the missing ratings through the matrix recovery process.\n",
        "\n",
        "![image.png](attachment:77dbbbed-eac6-48f7-9936-035ef3656349.png)"
      ],
      "metadata": {
        "id": "S1eQ1QHIhf6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "-----------"
      ],
      "metadata": {
        "id": "kuzEffLFhf6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸŽžï¸ Start To Build Recommendation Systems Based On Collaborative Filtering:"
      ],
      "metadata": {
        "id": "XzJmHvG8hf6m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---------------------"
      ],
      "metadata": {
        "id": "wYsAjHDehf6n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ—‚ï¸ Import necessary libraries"
      ],
      "metadata": {
        "id": "PMfMT-Uchf6n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:28:23.55452Z",
          "iopub.execute_input": "2024-10-11T01:28:23.555171Z",
          "iopub.status.idle": "2024-10-11T01:28:23.566886Z",
          "shell.execute_reply.started": "2024-10-11T01:28:23.555118Z",
          "shell.execute_reply": "2024-10-11T01:28:23.565368Z"
        },
        "trusted": true,
        "id": "5a7Ii1xihf6o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“ˆ Download DataSet"
      ],
      "metadata": {
        "id": "zgrgBWGahf6p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will use `MovieLens` dataset for my project, which is a popular dataset for movie ratings.\n",
        "I will download it from the GroupLens website."
      ],
      "metadata": {
        "id": "LpQZxunFhf6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! curl http://files.grouplens.org/datasets/movielens/ml-latest-small.zip -o ml-latest-small.zip"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:28:25.43808Z",
          "iopub.execute_input": "2024-10-11T01:28:25.438649Z",
          "iopub.status.idle": "2024-10-11T01:28:26.986955Z",
          "shell.execute_reply.started": "2024-10-11T01:28:25.438603Z",
          "shell.execute_reply": "2024-10-11T01:28:26.985353Z"
        },
        "trusted": true,
        "id": "ypLvdiSAhf6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# extracts the contents of the ml-latest-small.zip file to a directory named data.\n",
        "# This is useful when you need to access the files inside the ZIP archive.\n",
        "import zipfile\n",
        "with zipfile.ZipFile('ml-latest-small.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('data')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:28:29.038871Z",
          "iopub.execute_input": "2024-10-11T01:28:29.039461Z",
          "iopub.status.idle": "2024-10-11T01:28:29.078353Z",
          "shell.execute_reply.started": "2024-10-11T01:28:29.039407Z",
          "shell.execute_reply": "2024-10-11T01:28:29.077113Z"
        },
        "trusted": true,
        "id": "ppxw0XT6hf6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the dataset\n",
        "import pandas as pd\n",
        "movies_df = pd.read_csv('data/ml-latest-small/movies.csv')\n",
        "ratings_df = pd.read_csv('data/ml-latest-small/ratings.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:28:30.711896Z",
          "iopub.execute_input": "2024-10-11T01:28:30.71246Z",
          "iopub.status.idle": "2024-10-11T01:28:30.812138Z",
          "shell.execute_reply.started": "2024-10-11T01:28:30.712413Z",
          "shell.execute_reply": "2024-10-11T01:28:30.810797Z"
        },
        "trusted": true,
        "id": "P-hFL7mAhf6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“Š Data analysis with the MovieLens dataset."
      ],
      "metadata": {
        "id": "RimOT3F-hf6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('The dimensions of movies dataframe are:', movies_df.shape,'\\nThe dimensions of ratings dataframe are:', ratings_df.shape)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:28:33.758793Z",
          "iopub.execute_input": "2024-10-11T01:28:33.759325Z",
          "iopub.status.idle": "2024-10-11T01:28:33.766157Z",
          "shell.execute_reply.started": "2024-10-11T01:28:33.759277Z",
          "shell.execute_reply": "2024-10-11T01:28:33.764984Z"
        },
        "trusted": true,
        "id": "KE6kiOZ0hf6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "movies_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:28:35.15876Z",
          "iopub.execute_input": "2024-10-11T01:28:35.159301Z",
          "iopub.status.idle": "2024-10-11T01:28:35.175846Z",
          "shell.execute_reply.started": "2024-10-11T01:28:35.159253Z",
          "shell.execute_reply": "2024-10-11T01:28:35.174437Z"
        },
        "trusted": true,
        "id": "worcqS4Whf6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratings_df"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:28:36.06978Z",
          "iopub.execute_input": "2024-10-11T01:28:36.070373Z",
          "iopub.status.idle": "2024-10-11T01:28:36.087659Z",
          "shell.execute_reply.started": "2024-10-11T01:28:36.070327Z",
          "shell.execute_reply": "2024-10-11T01:28:36.08638Z"
        },
        "trusted": true,
        "id": "IBr-SPqRhf6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(movies_df.info())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:28:36.888605Z",
          "iopub.execute_input": "2024-10-11T01:28:36.889104Z",
          "iopub.status.idle": "2024-10-11T01:28:36.907279Z",
          "shell.execute_reply.started": "2024-10-11T01:28:36.889063Z",
          "shell.execute_reply": "2024-10-11T01:28:36.906026Z"
        },
        "trusted": true,
        "id": "gHv_FVjmhf6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ratings_df.info())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:28:37.539693Z",
          "iopub.execute_input": "2024-10-11T01:28:37.540231Z",
          "iopub.status.idle": "2024-10-11T01:28:37.555183Z",
          "shell.execute_reply.started": "2024-10-11T01:28:37.540184Z",
          "shell.execute_reply": "2024-10-11T01:28:37.553628Z"
        },
        "trusted": true,
        "id": "oGPJW_c_hf6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of ratings\n",
        "sns.histplot(ratings_df['rating'], bins=25, color='green', edgecolor='black')\n",
        "plt.title('Distribution of Ratings')\n",
        "plt.xlabel('Rating')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Sort unique values\n",
        "unique_ratings = np.sort(ratings_df['rating'].unique())\n",
        "\n",
        "# Modify the label on the X axis using ordered values\n",
        "plt.xticks(ticks=unique_ratings, labels=unique_ratings, fontsize=12, rotation=0)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:28:38.066355Z",
          "iopub.execute_input": "2024-10-11T01:28:38.067346Z",
          "iopub.status.idle": "2024-10-11T01:28:38.530863Z",
          "shell.execute_reply.started": "2024-10-11T01:28:38.067281Z",
          "shell.execute_reply": "2024-10-11T01:28:38.529706Z"
        },
        "trusted": true,
        "id": "zX9ox70Hhf6x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating some important statistics\n",
        "stats = {\n",
        "    'Total number of ratings': len(ratings_df),\n",
        "    'Average overall rating': ratings_df['rating'].mean(),\n",
        "    'Standard deviation of ratings': ratings_df['rating'].std(),\n",
        "    'Number of active users': len(ratings_df['userId'].unique()),\n",
        "    'Average number of ratings per user': len(ratings_df) / len(ratings_df['userId'].unique())\n",
        "}\n",
        "\n",
        "for key, value in stats.items():\n",
        "    print(f\"{key}: {value:.2f}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:28:43.655288Z",
          "iopub.execute_input": "2024-10-11T01:28:43.656164Z",
          "iopub.status.idle": "2024-10-11T01:28:43.667553Z",
          "shell.execute_reply.started": "2024-10-11T01:28:43.65612Z",
          "shell.execute_reply": "2024-10-11T01:28:43.666345Z"
        },
        "trusted": true,
        "id": "2RJnXSaehf6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Movie ID to movie name mapping\n",
        "movie_names = movies_df.set_index('movieId')['title'].to_dict()\n",
        "\n",
        "# Calculate the number of unique users and movies\n",
        "n_users = len(ratings_df.userId.unique())\n",
        "n_items = len(ratings_df.movieId.unique())\n",
        "\n",
        "# Print the results\n",
        "print(\"Number of unique users:\", n_users)\n",
        "print(\"Number of unique movies:\", n_items)\n",
        "print(\"The full rating matrix will have:\", n_users * n_items, 'elements.')\n",
        "print('---------------------------------------------------------------------')\n",
        "print(\"Number of ratings:\", len(ratings_df))\n",
        "print(\"Therefore: \", len(ratings_df) / (n_users * n_items) * 100, '% of the matrix is filled.')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:28:44.169952Z",
          "iopub.execute_input": "2024-10-11T01:28:44.170421Z",
          "iopub.status.idle": "2024-10-11T01:28:44.201225Z",
          "shell.execute_reply.started": "2024-10-11T01:28:44.17038Z",
          "shell.execute_reply": "2024-10-11T01:28:44.199798Z"
        },
        "trusted": true,
        "id": "854_AvC4hf6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have an incredibly sparse matrix to work with here.\n",
        "And... as the number of users and products grow, the number of elements will increase by **n*2**\n",
        "\n",
        "You are going to need a lot of memory to work with global scale... storing a full matrix in memory would be a challenge.\n",
        "One advantage here is that matrix factorization can realize the rating matrix implicitly, thus we don't need all the data."
      ],
      "metadata": {
        "id": "mntyvDBZhf60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:28:44.65911Z",
          "iopub.execute_input": "2024-10-11T01:28:44.660018Z",
          "iopub.status.idle": "2024-10-11T01:28:44.666673Z",
          "shell.execute_reply.started": "2024-10-11T01:28:44.659968Z",
          "shell.execute_reply": "2024-10-11T01:28:44.665386Z"
        },
        "trusted": true,
        "id": "EIjm-R5vhf61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“– Matrix Factorization, Model Initialization & Training Model"
      ],
      "metadata": {
        "id": "Uo8Y4DXWhf61"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Define Dataset\n",
        "class MovieDataset(Dataset):\n",
        "    def __init__(self, ratings_df, user_to_idx, movie_to_idx):\n",
        "        self.users = torch.tensor([user_to_idx[user] for user in ratings_df['userId']], dtype=torch.long)\n",
        "        self.movies = torch.tensor([movie_to_idx[movie] for movie in ratings_df['movieId']], dtype=torch.long)\n",
        "        self.ratings = torch.tensor(ratings_df['rating'].values, dtype=torch.float)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ratings)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.users[idx], self.movies[idx], self.ratings[idx]"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:28:45.461696Z",
          "iopub.execute_input": "2024-10-11T01:28:45.462261Z",
          "iopub.status.idle": "2024-10-11T01:28:45.472044Z",
          "shell.execute_reply.started": "2024-10-11T01:28:45.462187Z",
          "shell.execute_reply": "2024-10-11T01:28:45.470717Z"
        },
        "trusted": true,
        "id": "ZCalGwlEhf62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Define Matrix Factorization model\n",
        "class MatrixFactorization(nn.Module):\n",
        "    def __init__(self, n_users, n_movies, n_factors=50):\n",
        "        super().__init__()\n",
        "        self.user_factors = nn.Embedding(n_users, n_factors)\n",
        "        self.movie_factors = nn.Embedding(n_movies, n_factors)\n",
        "        self.user_biases = nn.Embedding(n_users, 1)\n",
        "        self.movie_biases = nn.Embedding(n_movies, 1)\n",
        "\n",
        "    def forward(self, user, movie):\n",
        "        user_embedding = self.user_factors(user)\n",
        "        movie_embedding = self.movie_factors(movie)\n",
        "        user_bias = self.user_biases(user)\n",
        "        movie_bias = self.movie_biases(movie)\n",
        "\n",
        "        prediction = (user_embedding * movie_embedding).sum(dim=1, keepdim=True)\n",
        "        prediction = prediction + user_bias + movie_bias\n",
        "        return prediction.squeeze()\n",
        "\n",
        "    def get_embeddings(self, user, movie):\n",
        "        \"\"\"Extract user and movie embeddings\"\"\"\n",
        "        user_embedding = self.user_factors(user)\n",
        "        movie_embedding = self.movie_factors(movie)\n",
        "        return user_embedding, movie_embedding"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:28:45.69949Z",
          "iopub.execute_input": "2024-10-11T01:28:45.700792Z",
          "iopub.status.idle": "2024-10-11T01:28:45.711212Z",
          "shell.execute_reply.started": "2024-10-11T01:28:45.700733Z",
          "shell.execute_reply": "2024-10-11T01:28:45.709922Z"
        },
        "trusted": true,
        "id": "TvgO6lG9hf63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Combined model with Matrix Factorization and K-means\n",
        "class CombinedRecommender:\n",
        "    def __init__(self, n_users, n_movies, n_factors=50, n_clusters=5):\n",
        "        self.mf_model = MatrixFactorization(n_users, n_movies, n_factors)\n",
        "        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "        self.n_factors = n_factors\n",
        "        self.n_clusters = n_clusters\n",
        "\n",
        "    def train_mf(self, train_loader, test_loader, n_epochs=10):\n",
        "        \"\"\"Train Matrix Factorization model\"\"\"\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = optim.Adam(self.mf_model.parameters(), lr=0.01)\n",
        "\n",
        "        train_losses = []\n",
        "        test_losses = []\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            # Train the model\n",
        "            self.mf_model.train()\n",
        "            total_train_loss = 0\n",
        "            for users, movies, ratings in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                predictions = self.mf_model(users, movies)\n",
        "                loss = criterion(predictions, ratings)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_train_loss += loss.item()\n",
        "\n",
        "            # Evaluate the model\n",
        "            self.mf_model.eval()\n",
        "            total_test_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for users, movies, ratings in test_loader:\n",
        "                    predictions = self.mf_model(users, movies)\n",
        "                    loss = criterion(predictions, ratings)\n",
        "                    total_test_loss += loss.item()\n",
        "\n",
        "            train_losses.append(total_train_loss / len(train_loader))\n",
        "            test_losses.append(total_test_loss / len(test_loader))\n",
        "\n",
        "            print(f'Epoch {epoch+1}/{n_epochs}')\n",
        "            print(f'Training Loss: {train_losses[-1]:.4f}')\n",
        "            print(f'Test Loss: {test_losses[-1]:.4f}')\n",
        "\n",
        "        return train_losses, test_losses\n",
        "\n",
        "    def get_combined_embeddings(self, users, movies):\n",
        "        \"\"\"Combine user and movie embeddings\"\"\"\n",
        "        self.mf_model.eval()\n",
        "        with torch.no_grad():\n",
        "            user_emb, movie_emb = self.mf_model.get_embeddings(users, movies)\n",
        "            combined_emb = torch.cat([user_emb, movie_emb], dim=1)\n",
        "            return combined_emb.numpy()\n",
        "\n",
        "    def train_kmeans(self, train_loader):\n",
        "        \"\"\"Train K-means on combined embeddings\"\"\"\n",
        "        all_embeddings = []\n",
        "        all_ratings = []\n",
        "\n",
        "        self.mf_model.eval()\n",
        "        with torch.no_grad():\n",
        "            for users, movies, ratings in train_loader:\n",
        "                combined_emb = self.get_combined_embeddings(users, movies)\n",
        "                all_embeddings.append(combined_emb)\n",
        "                all_ratings.extend(ratings.numpy())\n",
        "\n",
        "        all_embeddings = np.vstack(all_embeddings)\n",
        "        all_ratings = np.array(all_ratings)\n",
        "\n",
        "        # Train K-means\n",
        "        self.kmeans.fit(all_embeddings)\n",
        "        self.cluster_ratings = {}\n",
        "\n",
        "        # Calculate the average rating per cluster\n",
        "        clusters = self.kmeans.predict(all_embeddings)\n",
        "        for i in range(self.n_clusters):\n",
        "            self.cluster_ratings[i] = np.mean(all_ratings[clusters == i])\n",
        "\n",
        "    def predict(self, users, movies):\n",
        "        \"\"\"Predict using the combined model\"\"\"\n",
        "        combined_emb = self.get_combined_embeddings(users, movies)\n",
        "        clusters = self.kmeans.predict(combined_emb)\n",
        "\n",
        "        # Get predictions from Matrix Factorization\n",
        "        mf_predictions = self.mf_model(users, movies).numpy()\n",
        "\n",
        "        # Adjust predictions using cluster information\n",
        "        cluster_predictions = np.array([self.cluster_ratings[c] for c in clusters])\n",
        "\n",
        "        # Combine predictions (weights can be adjusted)\n",
        "        final_predictions = 0.7 * mf_predictions + 0.3 * cluster_predictions\n",
        "        return final_predictions"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:28:45.878582Z",
          "iopub.execute_input": "2024-10-11T01:28:45.879091Z",
          "iopub.status.idle": "2024-10-11T01:28:45.9008Z",
          "shell.execute_reply.started": "2024-10-11T01:28:45.879048Z",
          "shell.execute_reply": "2024-10-11T01:28:45.899641Z"
        },
        "trusted": true,
        "id": "2tckXJLfhf63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Training and evaluation\n",
        "def train_and_evaluate(ratings_df, n_factors=50, n_clusters=5):\n",
        "    # Prepare data\n",
        "    user_ids = ratings_df['userId'].unique()\n",
        "    movie_ids = ratings_df['movieId'].unique()\n",
        "\n",
        "    user_to_idx = {user: idx for idx, user in enumerate(user_ids)}\n",
        "    movie_to_idx = {movie: idx for idx, movie in enumerate(movie_ids)}\n",
        "\n",
        "    # Split data\n",
        "    train_df, test_df = train_test_split(ratings_df, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create DataLoaders\n",
        "    train_dataset = MovieDataset(train_df, user_to_idx, movie_to_idx)\n",
        "    test_dataset = MovieDataset(test_df, user_to_idx, movie_to_idx)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)\n",
        "\n",
        "    # Create and train the combined model\n",
        "    model = CombinedRecommender(len(user_ids), len(movie_ids), n_factors, n_clusters)\n",
        "\n",
        "    # 1. Train Matrix Factorization\n",
        "    print(\"Training Matrix Factorization...\")\n",
        "    train_losses, test_losses = model.train_mf(train_loader, test_loader)\n",
        "\n",
        "    # 2. Train K-means\n",
        "    print(\"\\nTraining K-means...\")\n",
        "    model.train_kmeans(train_loader)\n",
        "\n",
        "    # Evaluate the model\n",
        "    print(\"\\nEvaluating model...\")\n",
        "    model.mf_model.eval()\n",
        "    all_predictions = []\n",
        "    all_actuals = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for users, movies, ratings in test_loader:\n",
        "            predictions = model.predict(users, movies)\n",
        "            all_predictions.extend(predictions)\n",
        "            all_actuals.extend(ratings.numpy())\n",
        "\n",
        "    # Compute and display performance metrics\n",
        "    mse = mean_squared_error(all_actuals, all_predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    mae = mean_absolute_error(all_actuals, all_predictions)\n",
        "\n",
        "    print(f\"\\nFinal Results:\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    print(f\"MAE: {mae:.4f}\")\n",
        "\n",
        "    # Visualize the results\n",
        "    plt.figure(figsize=(15, 5))\n",
        "\n",
        "    # Plot learning curve\n",
        "    plt.subplot(1, 3, 1)\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(test_losses, label='Test Loss')\n",
        "    plt.title('Learning Curve')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot Predictions vs Actuals\n",
        "    plt.subplot(1, 3, 2)\n",
        "    plt.scatter(all_actuals, all_predictions, alpha=0.1)\n",
        "    plt.plot([1, 5], [1, 5], 'r--')\n",
        "    plt.title('Predictions vs Actuals')\n",
        "    plt.xlabel('Actual Ratings')\n",
        "    plt.ylabel('Predicted Ratings')\n",
        "\n",
        "    # Plot cluster distribution\n",
        "    plt.subplot(1, 3, 3)\n",
        "    combined_emb = model.get_combined_embeddings(\n",
        "        train_dataset.users[:1000],\n",
        "        train_dataset.movies[:1000]\n",
        "    )\n",
        "    clusters = model.kmeans.predict(combined_emb)\n",
        "\n",
        "    # Use t-SNE for dimensionality reduction\n",
        "    from sklearn.manifold import TSNE\n",
        "    tsne = TSNE(n_components=2, random_state=42)\n",
        "    reduced_emb = tsne.fit_transform(combined_emb)\n",
        "\n",
        "    plt.scatter(reduced_emb[:, 0], reduced_emb[:, 1], c=clusters, cmap='viridis')\n",
        "    plt.title('Cluster Distribution')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:28:46.114225Z",
          "iopub.execute_input": "2024-10-11T01:28:46.114687Z",
          "iopub.status.idle": "2024-10-11T01:28:46.134564Z",
          "shell.execute_reply.started": "2024-10-11T01:28:46.114645Z",
          "shell.execute_reply": "2024-10-11T01:28:46.133314Z"
        },
        "trusted": true,
        "id": "JJNon_rzhf64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ” Train The Model"
      ],
      "metadata": {
        "id": "UcWtXfophf7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = train_and_evaluate(ratings_df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:28:46.528946Z",
          "iopub.execute_input": "2024-10-11T01:28:46.529395Z",
          "iopub.status.idle": "2024-10-11T01:29:26.27781Z",
          "shell.execute_reply.started": "2024-10-11T01:28:46.529352Z",
          "shell.execute_reply": "2024-10-11T01:29:26.276462Z"
        },
        "trusted": true,
        "id": "xQobT700hf7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ’¿ Save The Model"
      ],
      "metadata": {
        "id": "JnwQrNElhf7K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "    'mf_model_state_dict': model.mf_model.state_dict(),\n",
        "    'kmeans_model': model.kmeans,\n",
        "    'cluster_ratings': model.cluster_ratings\n",
        "}, 'combined_recommender_model.pth')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:29:26.280405Z",
          "iopub.execute_input": "2024-10-11T01:29:26.281389Z",
          "iopub.status.idle": "2024-10-11T01:29:26.296173Z",
          "shell.execute_reply.started": "2024-10-11T01:29:26.281335Z",
          "shell.execute_reply": "2024-10-11T01:29:26.294776Z"
        },
        "trusted": true,
        "id": "eOdPtS12hf7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Let's also add a comprehensive evaluation that includes `confusion matrix` and additional analysis:"
      ],
      "metadata": {
        "id": "iHMfjCJOhf7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸš€ comprehensive evaluation"
      ],
      "metadata": {
        "id": "dxj4Zx5Dhf7L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedModelEvaluation:\n",
        "    def __init__(self, model, test_loader):\n",
        "        self.model = model\n",
        "        self.test_loader = test_loader\n",
        "        self.evaluate()\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"Comprehensive model evaluation\"\"\"\n",
        "        self.model.mf_model.eval()\n",
        "        self.predictions = []\n",
        "        self.actuals = []\n",
        "        self.embeddings = []\n",
        "        self.clusters = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for users, movies, ratings in self.test_loader:\n",
        "                # Get predictions and embeddings\n",
        "                preds = self.model.predict(users, movies)\n",
        "                emb = self.model.get_combined_embeddings(users, movies)\n",
        "                clusters = self.model.kmeans.predict(emb)\n",
        "\n",
        "                self.predictions.extend(preds)\n",
        "                self.actuals.extend(ratings.numpy())\n",
        "                self.embeddings.append(emb)\n",
        "                self.clusters.extend(clusters)\n",
        "\n",
        "        self.predictions = np.array(self.predictions)\n",
        "        self.actuals = np.array(self.actuals)\n",
        "        self.embeddings = np.vstack(self.embeddings)\n",
        "        self.clusters = np.array(self.clusters)\n",
        "\n",
        "    def create_confusion_matrix(self, threshold=3.5):\n",
        "        \"\"\"Create a confusion matrix using a specified threshold\"\"\"\n",
        "        pred_classes = (self.predictions >= threshold).astype(int)\n",
        "        actual_classes = (self.actuals >= threshold).astype(int)\n",
        "\n",
        "        conf_matrix = confusion_matrix(actual_classes, pred_classes)\n",
        "\n",
        "        plt.figure(figsize=(10, 8))\n",
        "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title('Confusion Matrix')\n",
        "        plt.xlabel('Predicted')\n",
        "        plt.ylabel('Actual')\n",
        "        plt.show()\n",
        "\n",
        "        # Print classification report\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(actual_classes, pred_classes))\n",
        "\n",
        "    def analyze_clusters(self):\n",
        "        \"\"\"Analyze clusters and their impact on performance\"\"\"\n",
        "        cluster_metrics = {}\n",
        "\n",
        "        for cluster in range(self.model.n_clusters):\n",
        "            # Get the indices of the data points in the current cluster\n",
        "            cluster_indices = np.where(self.clusters == cluster)[0]\n",
        "            cluster_predictions = self.predictions[cluster_indices]\n",
        "            cluster_actuals = self.actuals[cluster_indices]\n",
        "\n",
        "            # Calculate metrics for the current cluster\n",
        "            mse = mean_squared_error(cluster_actuals, cluster_predictions)\n",
        "            rmse = np.sqrt(mse)\n",
        "            mae = mean_absolute_error(cluster_actuals, cluster_predictions)\n",
        "\n",
        "            # Store metrics in the cluster_metrics dictionary\n",
        "            cluster_metrics[cluster] = {\n",
        "                'MSE': mse,\n",
        "                'RMSE': rmse,\n",
        "                'MAE': mae,\n",
        "                'Num Samples': len(cluster_indices)\n",
        "            }\n",
        "\n",
        "        # Print out cluster metrics\n",
        "        for cluster, metrics in cluster_metrics.items():\n",
        "            print(f\"\\nCluster {cluster} Metrics:\")\n",
        "            print(f\"Number of Samples: {metrics['Num Samples']}\")\n",
        "            print(f\"MSE: {metrics['MSE']:.4f}\")\n",
        "            print(f\"RMSE: {metrics['RMSE']:.4f}\")\n",
        "            print(f\"MAE: {metrics['MAE']:.4f}\")\n",
        "\n",
        "    def visualize_embeddings(self):\n",
        "        \"\"\"Visualize the embeddings of the users and movies using PCA\"\"\"\n",
        "        pca = PCA(n_components=2)\n",
        "        reduced_embeddings = pca.fit_transform(self.embeddings)\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        scatter = plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=self.clusters, cmap='viridis', alpha=0.6)\n",
        "        plt.title('PCA of User and Movie Embeddings')\n",
        "        plt.xlabel('PCA Component 1')\n",
        "        plt.ylabel('PCA Component 2')\n",
        "        plt.colorbar(scatter, label='Cluster')\n",
        "        plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:29:26.298217Z",
          "iopub.execute_input": "2024-10-11T01:29:26.298715Z",
          "iopub.status.idle": "2024-10-11T01:29:26.323887Z",
          "shell.execute_reply.started": "2024-10-11T01:29:26.298664Z",
          "shell.execute_reply": "2024-10-11T01:29:26.322734Z"
        },
        "trusted": true,
        "id": "Hb021GRXhf7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and test set\n",
        "train_df, test_df = train_test_split(ratings_df, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define user_to_idx and movie_to_idx based on the full dataset\n",
        "user_ids = ratings_df['userId'].unique()\n",
        "movie_ids = ratings_df['movieId'].unique()\n",
        "user_to_idx = {user: idx for idx, user in enumerate(user_ids)}\n",
        "movie_to_idx = {movie: idx for idx, movie in enumerate(movie_ids)}\n",
        "\n",
        "# Create a Dataset for the test set\n",
        "test_dataset = MovieDataset(test_df, user_to_idx, movie_to_idx)\n",
        "\n",
        "# Create a DataLoader for the test set\n",
        "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:29:26.327184Z",
          "iopub.execute_input": "2024-10-11T01:29:26.327693Z",
          "iopub.status.idle": "2024-10-11T01:29:26.376454Z",
          "shell.execute_reply.started": "2024-10-11T01:29:26.327641Z",
          "shell.execute_reply": "2024-10-11T01:29:26.375422Z"
        },
        "trusted": true,
        "id": "HsrnkVo0hf7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation = AdvancedModelEvaluation(model, test_loader)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:29:26.377801Z",
          "iopub.execute_input": "2024-10-11T01:29:26.378263Z",
          "iopub.status.idle": "2024-10-11T01:29:26.842377Z",
          "shell.execute_reply.started": "2024-10-11T01:29:26.378198Z",
          "shell.execute_reply": "2024-10-11T01:29:26.841255Z"
        },
        "trusted": true,
        "id": "xHvcIaRFhf7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation.create_confusion_matrix(threshold=3.5)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:29:26.844067Z",
          "iopub.execute_input": "2024-10-11T01:29:26.84447Z",
          "iopub.status.idle": "2024-10-11T01:29:27.229227Z",
          "shell.execute_reply.started": "2024-10-11T01:29:26.84443Z",
          "shell.execute_reply": "2024-10-11T01:29:27.228123Z"
        },
        "trusted": true,
        "id": "TYalMNqBhf7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation.analyze_clusters()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:29:27.230578Z",
          "iopub.execute_input": "2024-10-11T01:29:27.230934Z",
          "iopub.status.idle": "2024-10-11T01:29:27.244015Z",
          "shell.execute_reply.started": "2024-10-11T01:29:27.230896Z",
          "shell.execute_reply": "2024-10-11T01:29:27.242532Z"
        },
        "trusted": true,
        "id": "vsKwPdXGhf7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation.visualize_embeddings()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-10-11T01:29:27.245784Z",
          "iopub.execute_input": "2024-10-11T01:29:27.246149Z",
          "iopub.status.idle": "2024-10-11T01:29:28.519851Z",
          "shell.execute_reply.started": "2024-10-11T01:29:27.246111Z",
          "shell.execute_reply": "2024-10-11T01:29:28.51874Z"
        },
        "trusted": true,
        "id": "3laHHLnZhf7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¤— Thank You"
      ],
      "metadata": {
        "id": "vzC9jMRnhf7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "________"
      ],
      "metadata": {
        "id": "6RXYBpz3hf7O"
      }
    }
  ]
}